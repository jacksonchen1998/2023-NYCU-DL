{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import warnings\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe,vocab\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "1. How do you choose the tokenizer for this task? Could we use the white space to tokenize the text? What about using the complicated tokenizer instead? Make some discussion.\n",
    "\n",
    "I think the answer about the first question is yes, but the result may not be good. Since that the white space tokenizer will split the word by the white space, but the word may not be split by the white space.\n",
    "\n",
    "But the task in this task is enough to use the white space tokenizer.\n",
    "\n",
    "I choose `torchtext` with `basic_english` for the tokenizer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why we need the special tokens like ⟨pad⟩, ⟨unk⟩?\n",
    "\n",
    "Special tokens like ⟨pad⟩ and ⟨unk⟩ are used in natural language processing tasks to represent special cases that might occur in the data.\n",
    "- ⟨pad⟩: it is used to pad the sequence to the same length.\n",
    "- ⟨unk⟩: it is used to represent the word that is not in the vocabulary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Briefly explain how your procedure is run to handle the text data.\n",
    "\n",
    "- First, using `pandas` to read the data.\n",
    "- Second, combine the `headline` and `short_description` to `text`, and filter the stop words with `nltk`.\n",
    "- Thrid, using `torchtext` to build the vocabulary and tokenize the text.\n",
    "- Fourth, with `GloVe` to build the embedding matrix.\n",
    "- Fifth, build dataset and dataloader. And using `torch.nn.utils.rnn.pad_sequence` make sure the length of each batch is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nltk stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, category):\n",
    "        self.text = text\n",
    "        self.category = category\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.category[idx], self.text[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train.csv with pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "# combine headline and short_description into text, and remove idx\n",
    "df['text'] = df['headline'] + ' ' + df['short_description']\n",
    "# remove headline and short_description stopwords\n",
    "df[\"text_w\"] = df[\"text\"].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop_words)]))\n",
    "dataset = MyDataset(df['text_w'], df['category'])\n",
    "# split dataset into train and valid with random_split\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset)-int(len(dataset)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['text'] = test_df['headline'] + ' ' + test_df['short_description']\n",
    "test_df[\"text_w\"] = test_df[\"text\"].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop_words)]))\n",
    "test_dataset = TestDataset(test_df['text_w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec = GloVe(name='6B', dim=100)\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_pipeline = lambda x: text_vec.get_vecs_by_tokens(tokenizer(x), lower_case_backup=False)\n",
    "myvocab=vocab(text_vec.stoi, min_freq=0, specials=['<pad>','<unk>'], special_first = True) \n",
    "\n",
    "myvocab.set_default_index(myvocab['<unk>'])\n",
    "\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(myvocab(tokenizer(_text)), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, padding_value=0)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "def test_collate_batch(batch):\n",
    "    text_list = []\n",
    "    for (_text) in batch:\n",
    "         processed_text = torch.tensor(myvocab(tokenizer(_text)), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, padding_value=0)\n",
    "    return text_list.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "2. Discuss the model structure or hyperparameter setting in your design.\n",
    "\n",
    "Model structure:\n",
    "- Embedding layer: using the embedding matrix to get the embedding of each word. We called `encoder` and load the pretrained embedding matrix.\n",
    "- Positional encoding: using the positional encoding to add the position information to the embedding. We called `pos_encoder`.\n",
    "- Encoder: using encoder to encode the input sequence. We called `transformer_encoder`.\n",
    "- Using `torch.mean` to get the mean of the output of encoder.\n",
    "- Decoder: using decoder to decode the encoded sequence. After that we can get the `num_class` output. We called `decoder`.\n",
    "\n",
    "Hyperparameter setting:\n",
    "- `d_model` = 512\n",
    "- `d_hid` = 100\n",
    "- `nlayers` = 6\n",
    "- `nhead` = 4\n",
    "- `dropout` = 0.1\n",
    "\n",
    "The hyperparameter setting is the default setting in the `torch.nn.Transformer`.\n",
    "But `d_model` is 100. Since the embedding matrix is 100 dimension.\n",
    "And the `n_head` needs to be a factor of `d_model`, so I set it to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5, activatioln: str = 'relu') -> None:\n",
    "        super().__init__()\n",
    "        pretrain_emb=torch.cat([torch.zeros((2,100)),text_vec.vectors]) # add <pad> and <unk> to the pretrained embedding\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout,)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding.from_pretrained(pretrain_emb, freeze=False)\n",
    "        self.decoder = nn.Linear(d_model, num_class)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.encoder(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = torch.mean(output, dim=0)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=test_collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 4\n",
    "d_model = 100  # embedding dimension\n",
    "d_hid = 400  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 6  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 4  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.1  # dropout probability\n",
    "total_epoch = 100\n",
    "act_funct = 'silu'\n",
    "\n",
    "model = TransformerModel(num_class, d_model, nhead, d_hid, nlayers, dropout, activatioln=act_funct).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epoch, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc, best_loss = 0, 1e9\n",
    "\n",
    "for i in range(total_epoch):\n",
    "    train_acc, train_loss = 0, 0\n",
    "    valid_acc, valid_loss = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    train_loop = tqdm.tqdm((train_dataloader), total=len(train_dataloader))\n",
    "    for idx, (label, text) in enumerate(train_loop):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred = torch.argmax(F.softmax(output, dim=1) ,dim=1)\n",
    "        train_acc += (pred == label).sum().item()\n",
    "        train_loop.set_description(f\"Epoch [{i+1}/{total_epoch}]\")\n",
    "        train_loop.set_postfix(train_loss=train_loss/(len(train_dataloader) * batch), train_acc=train_acc/(len(train_dataloader) * batch))\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    valid_loop = tqdm.tqdm((valid_dataloader), total=len(valid_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(valid_loop):\n",
    "            output = model(text)\n",
    "            loss = criterion(output, label)\n",
    "            valid_loss += loss.item()\n",
    "            pred = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "            valid_acc += (pred == label).sum().item()\n",
    "            valid_loop.set_description(f\"Epoch [{i+1}/{total_epoch}]\")\n",
    "            valid_loop.set_postfix(valid_loss=valid_loss/(len(valid_dataloader) * batch), valid_acc=valid_acc/(len(valid_dataloader) * batch))\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_loss_model.pth')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_state_dict(torch.load('best_loss_model.pth'))\n",
    "\n",
    "pred = np.zeros(len(test_dataloader))\n",
    "index = 0\n",
    "\n",
    "test_loop = tqdm.tqdm((test_dataloader), total=len(test_dataloader))\n",
    "for idx, (text) in enumerate(test_loop):\n",
    "    with torch.no_grad():\n",
    "        output = model(text)\n",
    "        pred[idx] = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "    index += 1\n",
    "\n",
    "with open('result.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'category'])\n",
    "    for i in range(len(pred)):\n",
    "        writer.writerow([i+1, int(pred[i]) + 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
