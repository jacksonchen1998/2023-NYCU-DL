{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenize the given text data with some off-the-shelf software.\n",
    "2. Build the vocabulary (like the dictionary object in python) to map the token into some unique ID.\n",
    "3. Select the pretrained embedding (Glove, Fasttext, Word2vec) as the initialization of your embedding layer. (Not necessary, but recommended)\n",
    "4. Construct your transformer model and finally end up with some simple feed forward module.\n",
    "5. Choose the suitable optimizer (Adam might be not suitable) and activation function (ReLU might be not suitable. Try Tanh or Swish?)\n",
    "6. Try some tricks like learning rate scheduler.\n",
    "7. Check some tutorial such as [Here](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the headline and the content of the news, you need to train a model to correctly classify the news into 4 different category:\n",
    "1. Sports\n",
    "2. Business\n",
    "3. Tech\n",
    "4. Media\n",
    "\n",
    "train.csv contains 4 columns:\n",
    "id,category,headline,short_description\n",
    "\n",
    "test.csv contains 3 columns:\n",
    "id,headline,short_description\n",
    "\n",
    "submission.csv contains 2 columns:\n",
    "id,category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# read train.csv\n",
    "with open('train.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        data.append(row)\n",
    "    data = data[1:]\n",
    "\n",
    "# read test.csv\n",
    "with open('test.csv', newline='') as csvfile:\n",
    "    rows = csv.reader(csvfile)\n",
    "    test = []\n",
    "    for row in rows:\n",
    "        test.append(row)\n",
    "    test = test[1:]\n",
    "\n",
    "# split train.csv into train and valid\n",
    "train = data[:int(len(data)*0.8)]\n",
    "valid = data[int(len(data)*0.8):]\n",
    "\n",
    "# get the category of train, valid, and test\n",
    "train_category = [row[1] for row in train]\n",
    "valid_category = [row[1] for row in valid]\n",
    "test_category = [row[1] for row in test]\n",
    "\n",
    "# get the headline and short_description of train, valid, and test\n",
    "train_headline = [row[2] for row in train]\n",
    "valid_headline = [row[2] for row in valid]\n",
    "test_headline = [row[1] for row in test]\n",
    "train_short_description = [row[3] for row in train]\n",
    "valid_short_description = [row[3] for row in valid]\n",
    "test_short_description = [row[2] for row in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mllab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_headline = [[word for word in headline.split() if word not in stop_words] for headline in train_headline]\n",
    "valid_headline = [[word for word in headline.split() if word not in stop_words] for headline in valid_headline]\n",
    "test_headline = [[word for word in headline.split() if word not in stop_words] for headline in test_headline]\n",
    "train_short_description = [[word for word in short_description.split() if word not in stop_words] for short_description in train_short_description]\n",
    "valid_short_description = [[word for word in short_description.split() if word not in stop_words] for short_description in valid_short_description]\n",
    "test_short_description = [[word for word in short_description.split() if word not in stop_words] for short_description in test_short_description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine headline and short_description\n",
    "train_data = [train_headline[i] + train_short_description[i] for i in range(len(train_headline))]\n",
    "valid_data = [valid_headline[i] + valid_short_description[i] for i in range(len(valid_headline))]\n",
    "test_data = [test_headline[i] + test_short_description[i] for i in range(len(test_headline))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_vocab_from_iterator() got an unexpected keyword argument 'specials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_165763/322348920.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'basic_english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalid_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: build_vocab_from_iterator() got an unexpected keyword argument 'specials'"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_vocab = build_vocab_from_iterator(map(tokenizer, train_data), specials=[\"<unk>\"])\n",
    "valid_vocab = build_vocab_from_iterator(map(tokenizer, valid_data), specials=[\"<unk>\"])\n",
    "\n",
    "train_vocab.set_default_index(train_vocab[\"<unk>\"])\n",
    "valid_vocab.set_default_index(valid_vocab[\"<unk>\"])\n",
    "\n",
    "train_text_pipeline = lambda x: train_vocab(tokenizer(x))\n",
    "valid_text_pipeline = lambda x: valid_vocab(tokenizer(x))\n",
    "\n",
    "train_label_pipeline = lambda x: int(x) - 1\n",
    "valid_label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(train_label_pipeline(_label))\n",
    "        processed_text = torch.tensor(train_text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "def valid_collate_fn(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(valid_label_pipeline(_label))\n",
    "        processed_text = torch.tensor(valid_text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=64, shuffle=True, collate_fn=train_collate_fn)\n",
    "valid_dataloader = DataLoader(valid, batch_size=64, shuffle=True, collate_fn=valid_collate_fn)\n",
    "    \n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
