{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenize the given text data with some off-the-shelf software.\n",
    "2. Build the vocabulary (like the dictionary object in python) to map the token into some unique ID.\n",
    "3. Select the pretrained embedding (Glove, Fasttext, Word2vec) as the initialization of your embedding layer. (Not necessary, but recommended)\n",
    "4. Construct your transformer model and finally end up with some simple feed forward module.\n",
    "5. Choose the suitable optimizer (Adam might be not suitable) and activation function (ReLU might be not suitable. Try Tanh or Swish?)\n",
    "6. Try some tricks like learning rate scheduler.\n",
    "7. Check some tutorial such as [Here](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the headline and the content of the news, you need to train a model to correctly classify the news into 4 different category:\n",
    "1. Sports\n",
    "2. Business\n",
    "3. Tech\n",
    "4. Media\n",
    "\n",
    "train.csv contains 4 columns:\n",
    "id,category,headline,short_description\n",
    "\n",
    "test.csv contains 3 columns:\n",
    "id,headline,short_description\n",
    "\n",
    "submission.csv contains 2 columns:\n",
    "id,category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import warnings\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nltk stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, category):\n",
    "        self.text = text\n",
    "        self.category = category\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.category[idx], self.text[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train.csv with pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "# combine headline and short_description into text, and remove idx\n",
    "df['text'] = df['headline'] + ' ' + df['short_description']\n",
    "# remove headline and short_description stopwords\n",
    "df[\"text_w\"] = df[\"text\"].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop_words)]))\n",
    "dataset = MyDataset(df['text_w'], df['category'])\n",
    "# split dataset into train and valid with random_split\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset)-int(len(dataset)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['text'] = test_df['headline'] + ' ' + test_df['short_description']\n",
    "test_df[\"text_w\"] = test_df[\"text\"].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop_words)]))\n",
    "test_dataset = TestDataset(test_df['text_w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec = GloVe(name='6B', dim=100)\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: text_vec.get_vecs_by_tokens(tokenizer(x), lower_case_backup=False)\n",
    "\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, padding_value=0)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "def test_collate_batch(batch):\n",
    "    text_list = []\n",
    "    for (_text) in batch:\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, padding_value=0)\n",
    "    return text_list.to(device)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5, activatioln: str = 'relu') -> None:\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout,)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding.from_pretrained(text_vec.vectors, freeze=False)\n",
    "        self.decoder = nn.Linear(d_model, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = torch.mean(output, dim=0)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=test_collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 4\n",
    "emsize = 100  # embedding dimension\n",
    "d_hid = 400  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 4  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.1  # dropout probability\n",
    "total_epoch = 10\n",
    "act_funct = 'silu'\n",
    "\n",
    "model = TransformerModel(num_class, emsize, nhead, d_hid, nlayers, dropout, activatioln=act_funct).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epoch, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc, best_loss = 0, 1e9\n",
    "\n",
    "for i in range(total_epoch):\n",
    "    train_acc, train_loss = 0, 0\n",
    "    valid_acc, valid_loss = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    train_loop = tqdm.tqdm((train_dataloader), total=len(train_dataloader))\n",
    "    for idx, (label, text) in enumerate(train_loop):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred = torch.argmax(F.softmax(output, dim=1) ,dim=1)\n",
    "        # print(\"train_pred\", pred)\n",
    "        # print(\"train_label\", label)\n",
    "        train_acc += (pred == label).sum().item()\n",
    "        train_loop.set_description(f\"Epoch [{i+1}/{total_epoch}]\")\n",
    "        train_loop.set_postfix(train_loss=train_loss/(len(train_dataloader) * batch), train_acc=train_acc/(len(train_dataloader) * batch))\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    valid_loop = tqdm.tqdm((valid_dataloader), total=len(valid_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(valid_loop):\n",
    "            output = model(text)\n",
    "            loss = criterion(output, label)\n",
    "            valid_loss += loss.item()\n",
    "            pred = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "            # print(\"valid_pred\", pred)\n",
    "            # print(\"valid_label\", label)\n",
    "            valid_acc += (pred == label).sum().item()\n",
    "            valid_loop.set_description(f\"Epoch [{i+1}/{total_epoch}]\")\n",
    "            valid_loop.set_postfix(valid_loss=valid_loss/(len(valid_dataloader) * batch), valid_acc=valid_acc/(len(valid_dataloader) * batch))\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_loss_model.pth')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_state_dict(torch.load('best_loss_model.pth'))\n",
    "\n",
    "pred = np.zeros(len(test_dataloader))\n",
    "index = 0\n",
    "\n",
    "test_loop = tqdm.tqdm((test_dataloader), total=len(test_dataloader))\n",
    "for idx, (text) in enumerate(test_loop):\n",
    "    with torch.no_grad():\n",
    "        output = model(text)\n",
    "        pred[idx] = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "    index += 1\n",
    "\n",
    "with open('result.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'category'])\n",
    "    for i in range(len(pred)):\n",
    "        writer.writerow([i+1, int(pred[i]) + 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
