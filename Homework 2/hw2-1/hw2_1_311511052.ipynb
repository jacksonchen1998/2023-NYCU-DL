{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a7FxF6t_e0sA"
      },
      "source": [
        "# Image Generation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bkeeZ2rwe9uc"
      },
      "source": [
        "## 1.1 Generative adversarial network\n",
        "#### In this exercise, you will implement a Deep Convolutional Generative Network (DCGAN) to synthesis images by using the provided anime faces dataset.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- Construct a <font color=red>DCGAN</font> with GAN objective, you can refer to the [tutorial website](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) provided by PyTorch for implementation.\n",
        "$$\n",
        "    \\begin{equation*} \\begin{aligned}\n",
        "    &\\max _{D} \\mathcal{L}(D) =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}} \\log D(\\boldsymbol{x})+\\mathbb{E}_{z \\sim p_{\\boldsymbol{z}}} \\log (1-D(G(\\boldsymbol{z}))) \\\\\n",
        "    &\\min _{G} \\mathcal{L}(G) =\\mathbb{E}_{z \\sim p_{x}} \\log (1-D(G(\\boldsymbol{z}))\n",
        "    \\end{aligned} \\end{equation*}\n",
        "$$\n",
        "- <font color=red>Draw</font> some samples generated from your generator at <font color=red>different training stages </font>. For example, you may show the results when running at $5^{\\text{th}}$ and final epoch 100. (10\\%)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "paxpB-Emh93P"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://i.imgur.com/tnRR3tr.png\" width=\"350px\" />\n",
        "    <img src=\"https://i.imgur.com/g9AnDwN.png\" width=\"350px\" />\n",
        "</center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIC7ZXlfrwsB"
      },
      "outputs": [],
      "source": [
        "# # Downlaod and unzip data\n",
        "# !gdown 1K1oB7GOUerTCIa68bbxETcGajLeE_5j1\n",
        "# !unzip resized_64x64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "import tqdm as tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ko0sKvBby1o8"
      },
      "source": [
        "### Please write the gan code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdkJu9NLkGFn"
      },
      "outputs": [],
      "source": [
        "# Please write the gan code here\n",
        "# Note: In our experience, you can just select around 10000 images for training and get acceptable result.\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "data_path = './data/'\n",
        "workers = 2\n",
        "batch_size = 128\n",
        "image_size = 64\n",
        "\n",
        "num_channels = 3\n",
        "num_latents = 100\n",
        "size_feature_map_gen = 64\n",
        "size_feature_map_disc = 64\n",
        "train_epochs = 5\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "workers = 0\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(\n",
        "    root=data_path,\n",
        "    transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(image_size),\n",
        "        torchvision.transforms.CenterCrop(image_size),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])\n",
        ")\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == torch.nn.Conv2d or type(m) == torch.nn.ConvTranspose2d:\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    elif type(m) == torch.nn.BatchNorm2d:\n",
        "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Genterator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Genterator, self).__init__()\n",
        "        self.main = torch.nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            torch.nn.ConvTranspose2d(num_latents, size_feature_map_gen * 8, 4, 1, 0, bias=False),\n",
        "            torch.nn.BatchNorm2d(size_feature_map_gen * 8),\n",
        "            torch.nn.ReLU(True),\n",
        "            # state size. (size_feature_map_gen*8) x 4 x 4\n",
        "            torch.nn.ConvTranspose2d(size_feature_map_gen * 8, size_feature_map_gen * 4, 4, 2, 1, bias=False),\n",
        "            torch.nn.BatchNorm2d(size_feature_map_gen * 4),\n",
        "            torch.nn.ReLU(True),\n",
        "            # state size. (size_feature_map_gen*4) x 8 x 8\n",
        "            torch.nn.ConvTranspose2d(size_feature_map_gen * 4, size_feature_map_gen * 2, 4, 2, 1, bias=False),\n",
        "            torch.nn.BatchNorm2d(size_feature_map_gen * 2),\n",
        "            torch.nn.ReLU(True),\n",
        "            # state size. (size_feature_map_gen*2) x 16 x 16\n",
        "            torch.nn.ConvTranspose2d(size_feature_map_gen * 2, size_feature_map_gen, 4, 2, 1, bias=False),\n",
        "            torch.nn.BatchNorm2d(size_feature_map_gen),\n",
        "            torch.nn.ReLU(True),\n",
        "            # state size. (size_feature_map_gen) x 32 x 32\n",
        "            torch.nn.ConvTranspose2d(size_feature_map_gen, num_channels, 4, 2, 1, bias=False),\n",
        "            torch.nn.Tanh()\n",
        "            # state size. (num_channels) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "    \n",
        "G_net = Genterator().to(device)\n",
        "G_net.apply(init_weights)\n",
        "print(G_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = torch.nn.Sequential(\n",
        "            # input is (num_channels) x 64 x 64\n",
        "            torch.nn.Conv2d(num_channels, size_feature_map_disc, 4, 2, 1, bias=False),\n",
        "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (size_feature_map_disc) x 32 x 32\n",
        "            torch.nn.Conv2d(size_feature_map_disc, size_feature_map_disc * 2, 4, 2, 1, bias=False),\n",
        "            torch.nn.BatchNorm2d(size_feature_map_disc * 2),\n",
        "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (size_feature_map_disc*2) x 16 x 16\n",
        "            torch.nn.Conv2d(size_feature_map_disc * 2, size_feature_map_disc * 4, 4, 2, 1, bias=False),\n",
        "            torch.nn.BatchNorm2d(size_feature_map_disc * 4),\n",
        "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (size_feature_map_disc*4) x 8 x 8\n",
        "            torch.nn.Conv2d(size_feature_map_disc * 4, size_feature_map_disc * 8, 4, 2, 1, bias=False),\n",
        "            torch.nn.BatchNorm2d(size_feature_map_disc * 8),\n",
        "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (size_feature_map_disc*8) x 4 x 4\n",
        "            torch.nn.Conv2d(size_feature_map_disc * 8, 1, 4, 1, 0, bias=False),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "    \n",
        "D_net = Discriminator().to(device)\n",
        "D_net.apply(init_weights)\n",
        "print(D_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = torch.nn.BCELoss()\n",
        "fixed_noise = torch.randn(64, num_latents, 1, 1, device=device)\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "d_optimizer = torch.optim.Adam(D_net.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "g_optimizer = torch.optim.Adam(G_net.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "temp_epochs, final_epochs = 5, 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(temp_epochs):\n",
        "    temp_gloss, temp_dloss = 0, 0\n",
        "    with tqdm.tqdm(total=len(dataloader)) as pbar:\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "            D_net.zero_grad()\n",
        "\n",
        "            real_cpu = data[0].to(device)\n",
        "            b_size = real_cpu.size(0)\n",
        "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "\n",
        "            output = D_net(real_cpu).view(-1)\n",
        "\n",
        "            errD_real = criterion(output, label)\n",
        "\n",
        "            errD_real.backward()\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            noise = torch.randn(b_size, num_latents, 1, 1, device=device)\n",
        "\n",
        "            fake = G_net(noise)\n",
        "            label.fill_(fake_label)\n",
        "            \n",
        "            output = D_net(fake.detach()).view(-1)\n",
        "            errD_fake = criterion(output, label)\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "            errD = errD_real + errD_fake\n",
        "            d_optimizer.step()\n",
        "\n",
        "            G_net.zero_grad()\n",
        "            label.fill_(real_label)\n",
        "            output = D_net(fake).view(-1)\n",
        "            errG = criterion(output, label)\n",
        "            errG.backward()\n",
        "            D_G_z2 = output.mean().item()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            temp_gloss += errG.item()\n",
        "            temp_dloss += errD.item()\n",
        "\n",
        "            if (iters % 500 == 0) or ((epoch == temp_epochs-1) and (i == len(dataloader)-1)):\n",
        "                with torch.no_grad():\n",
        "                    fake = G_net(fixed_noise).detach().cpu()\n",
        "                img_list.append(torchvision.utils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "            iters += 1\n",
        "\n",
        "            pbar.set_description(f\"Epoch {epoch}\")\n",
        "            pbar.set_postfix(Loss_D=errD.item(), Loss_G=errG.item(), D_x=D_x, D_G_z1=D_G_z1, D_G_z2=D_G_z2)\n",
        "            pbar.update(1)\n",
        "\n",
        "        G_losses.append(temp_gloss/(len(dataloader) * batch_size))\n",
        "        D_losses.append(temp_dloss/(len(dataloader) * batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training with 5 epochs\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Binary Cross Entropy Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Fake Images with 5 Epochs\")\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.savefig(\"fake_images_5.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize generator and discriminator\n",
        "G_net.apply(init_weights)\n",
        "D_net.apply(init_weights)\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "img_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(final_epochs):\n",
        "    temp_gloss, temp_dloss = 0, 0\n",
        "    with tqdm.tqdm(total=len(dataloader)) as pbar:\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "            D_net.zero_grad()\n",
        "\n",
        "            real_cpu = data[0].to(device)\n",
        "            b_size = real_cpu.size(0)\n",
        "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "\n",
        "            output = D_net(real_cpu).view(-1)\n",
        "\n",
        "            errD_real = criterion(output, label)\n",
        "\n",
        "            errD_real.backward()\n",
        "            D_x = output.mean().item()\n",
        "\n",
        "            noise = torch.randn(b_size, num_latents, 1, 1, device=device)\n",
        "\n",
        "            fake = G_net(noise)\n",
        "            label.fill_(fake_label)\n",
        "            \n",
        "            output = D_net(fake.detach()).view(-1)\n",
        "            errD_fake = criterion(output, label)\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "            errD = errD_real + errD_fake\n",
        "            d_optimizer.step()\n",
        "\n",
        "            G_net.zero_grad()\n",
        "            label.fill_(real_label)\n",
        "            output = D_net(fake).view(-1)\n",
        "            errG = criterion(output, label)\n",
        "            errG.backward()\n",
        "            D_G_z2 = output.mean().item()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            temp_gloss += errG.item()\n",
        "            temp_dloss += errD.item()\n",
        "\n",
        "            if (iters % 500 == 0) or ((epoch == temp_epochs-1) and (i == len(dataloader)-1)):\n",
        "                with torch.no_grad():\n",
        "                    fake = G_net(fixed_noise).detach().cpu()\n",
        "                img_list.append(torchvision.utils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "            iters += 1\n",
        "\n",
        "            pbar.set_description(f\"Epoch {epoch}\")\n",
        "            pbar.set_postfix(Loss_D=errD.item(), Loss_G=errG.item(), D_x=D_x, D_G_z1=D_G_z1, D_G_z2=D_G_z2)\n",
        "            pbar.update(1)\n",
        "\n",
        "        G_losses.append(temp_gloss/(len(dataloader) * batch_size))\n",
        "        D_losses.append(temp_dloss/(len(dataloader) * batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training with 100 epochs\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Binary Cross Entropy Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"losses_100.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Fake Images with 100 Epochs\")\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.savefig(\"fake_images_100.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "HTML(ani.to_jshtml())\n",
        "ani.save(\"animation.gif\", writer=\"imagemagick\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d1pz3VO-y43o"
      },
      "source": [
        "### 1.1.a \n",
        "### Draw some samples generated from your generator at different training stages. For example, you may show the results when running at 5th and final epoch 100"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0ULIT86Dlts4"
      },
      "source": [
        "<center>\n",
        "    <img src = \"./image/fake_images_5.png\" width=\"49%\">\n",
        "    <img src = \"./image/fake_images_100.png\" width=\"49%\">\n",
        "</center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Opesk7JWy_3M"
      },
      "source": [
        "### 1.1.b\n",
        "### The Helvetica Scenario often happens during the training procedure of GAN. Please explain why this problem occurs and how to avoid it. We suggest you can read the original paper and do the discuss.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HV49xflDltIS"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TaZzYCrRkMkP"
      },
      "source": [
        "## 1.2 Denoising Diffusion Probabilistic Model (30%)\n",
        "\n",
        "#### In this exercise, you will implement a <font color=red>Denoising Diffusion Probabilistic Model (DDPM) </font>to generate images by the provided  <font color=red>anime faces dataset</font>. The Figure below is the process of the Diffusion Model. It consists of a forward process, which gradually adds noise, and the reverse process will transform the noise back into a sample from the target distribution. Here is the [link1](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) and [link2](https://www.youtube.com/watch?v=azBugJzmz-o&t=190s) to the detailed introduction to the diffusion model. \n",
        "\n",
        "<center>\n",
        "  <img src=\"https://i.imgur.com/BqpRi4v.png\"/>\n",
        "</center>\n",
        "\n",
        "1. Construct  <font color='blue'>DDPM</font> by fulfilling the <font color='red'>2 TODOs</font> and follow the instruction. Noticed that you are not allowed to directly call library or API to load the model. The total epoch is 10. (20\\%)\n",
        "\n",
        "  (a) **Draw** some generated samples based on diffusion steps $T = 500$ and $T = 1000$. We provide the **pre-trained weights** which are trained with 500 and 1000 steps. Hint: In the paper, the steps start at 1..\n",
        "\n",
        "  (b) **Discuss** the result based on different diffusion steps."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YltKVw2rnIIZ"
      },
      "source": [
        "### Training (You can skip this)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oaolH162cgAk"
      },
      "source": [
        "- Notice that becuase the diffusion requires high computational device, Colab may not be suitable. Thus, we provide the code of Training for reference. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIl_Nse5nktR",
        "outputId": "66307e1f-310e-44e1-e0f8-58bcb6803b4b"
      },
      "outputs": [],
      "source": [
        "# !gdown 1E8yulcTDMk9dvz2dJ_TniLKdU4n6AFwa\n",
        "# !gdown 1g_RYSP1A2rXg_ud18ARlWXK8BWhiHdjV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuuLfEH-pNDQ",
        "outputId": "b2f628c8-afbf-4946-fad8-4497b451013e"
      },
      "outputs": [],
      "source": [
        "# !pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB83iewwnLG_",
        "outputId": "e34de929-3b98-4f23-fdc8-89e1251eb7ce"
      },
      "outputs": [],
      "source": [
        "import torch, sys\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm, trange\n",
        "from model import Unet\n",
        "from torchmetrics import MeanMetric\n",
        "from dataloader import get_loader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device) #make sure this is cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxpMQGFEoCMu",
        "outputId": "d37b9e80-9e22-4fd3-fae3-b304a563511d"
      },
      "outputs": [],
      "source": [
        "T = 500\n",
        "ALPHA = 1-torch.linspace(1e-4, 2e-2, T)\n",
        "def alpha(t):\n",
        "    at = torch.prod(ALPHA[:t]).reshape((1, ))\n",
        "    return torch.sqrt(torch.cat((at, 1-at)))\n",
        "ALPHA_bar = torch.stack([alpha(t) for t in range(T)]).to(device)\n",
        "\n",
        "batch_size = 32\n",
        "update_step = 1\n",
        "save_step = 2\n",
        "save_step_ = 20\n",
        "num_workers = 6\n",
        "epochs = 100\n",
        "loss_func = torch.nn.MSELoss()\n",
        "lr = 5e-4\n",
        "model = Unet(\n",
        "    in_channels=3\n",
        ")\n",
        "state_dict = torch.load('checkpoint_100epoch_T500.pth')\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nogEBAVtoCTW"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader):\n",
        "    running_loss = MeanMetric(accumulate=True)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for epoch in (overall:=trange(1, epochs+1, position=1, desc='[Overall]')):\n",
        "        running_loss.reset()\n",
        "\n",
        "        for i, X_0 in enumerate(bar := tqdm(data_loader, position=0, desc=f'[Train {epoch:3d}] lr={scheduler.get_last_lr()[0]:.2e}'), start=1):\n",
        "            X_0 = X_0.to(device)\n",
        "            eps = torch.randn(X_0.shape, device=device)\n",
        "            t = torch.randint(0, T, (X_0.shape[0], ), device=device)\n",
        "\n",
        "            # print(ALPHA_bar[t, 0].reshape(-1, 1, 1, 1)*X_0)\n",
        "            with torch.no_grad():\n",
        "                X_noise = ALPHA_bar[t, 0].reshape(-1, 1, 1, 1)*X_0 + ALPHA_bar[t, 1].reshape(-1, 1, 1, 1)*eps\n",
        "            # X_noise = X_noise.to(device)\n",
        "            t = t.to(device)\n",
        "            \n",
        "            pred = model(X_noise, t+1)\n",
        "\n",
        "            loss = loss_func(eps, pred)\n",
        "            loss.backward()\n",
        "\n",
        "            if i%update_step == 0 or i == bar.total:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            running_loss.update(loss.item())\n",
        "            bar.set_postfix_str(f'loss {running_loss.compute():.2e}')\n",
        "\n",
        "        scheduler.step()\n",
        "        tqdm.write('\\r\\033[K', end='')\n",
        "\n",
        "        if epoch % save_step == 0:\n",
        "            save_checkpoint(epoch, model, optimizer, 'checkpoint.pth')\n",
        "        if epoch % save_step_ == 0:\n",
        "            save_checkpoint(epoch, model, optimizer, f'checkpoint_{epoch}.pth')    \n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict()\n",
        "    }, path)\n",
        "    tqdm.write('Save checkpoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13W3fRwMtysf"
      },
      "outputs": [],
      "source": [
        "# Training start\n",
        "train_loader = get_loader(\n",
        "    './data/resized_64x64/',\n",
        "    batch_size=batch_size, \n",
        "    num_workers=num_workers,\n",
        ")\n",
        "model = model.to(device)\n",
        "train(model, train_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bh7YWsmonLnL"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7wSw6tVdpTm",
        "outputId": "f6deae63-b130-460d-849f-fd3c87bfb94f"
      },
      "outputs": [],
      "source": [
        "# !gdown 1E8yulcTDMk9dvz2dJ_TniLKdU4n6AFwa \n",
        "# # dataloader.py\n",
        "# !gdown 1g_RYSP1A2rXg_ud18ARlWXK8BWhiHdjV \n",
        "# # model.py\n",
        "# !gdown 1n9K-HSY3GJKTS4HkHTCA_AJT0q1cKBZ1 \n",
        "# # checkpoint_epoch100_T1000.pth\n",
        "# !gdown 1jPycQFo_f_fPRUg6OuauTrsXbdibTvKI \n",
        "# # checkpoint_epoch100_T500.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJzQj21gld26",
        "outputId": "2294066d-1a65-4e15-80e0-bd682f0ac063"
      },
      "outputs": [],
      "source": [
        "import torch, os\n",
        "from tqdm import trange, tqdm\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.transforms import ColorJitter\n",
        "import torch, sys\n",
        "from tqdm import tqdm, trange\n",
        "from model import Unet\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device) # cuda is recommand\n",
        "T1 = 500   # 500 or 1000\n",
        "model1 = Unet(in_channels=3).to(device)\n",
        "state_dict1 = torch.load('checkpoint_100epoch_T500.pth')\n",
        "model1.load_state_dict(state_dict1['model_state_dict'])\n",
        "\n",
        "T2 = 1000   # 500 or 1000\n",
        "model2 = Unet(in_channels=3).to(device)\n",
        "state_dict2 = torch.load('checkpoint_100epoch_T1000.pth')\n",
        "model2.load_state_dict(state_dict2['model_state_dict'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center>\n",
        "    <img src = \"./image/DDPM.png\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfh7leoPqyK1"
      },
      "outputs": [],
      "source": [
        "ALPHA = (1-torch.linspace(1e-4, 2e-2, T)).reshape((-1, 1)).to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_and_save(model, gen_N, chan=3, resolu=(28, 28)):\n",
        "    model.eval()\n",
        "    L = []\n",
        "    # Sample gaussian noise X_T (5%) and denoise it\n",
        "    X_T = torch.randn(gen_N, chan, *resolu, device=device)\n",
        "    L.append(X_T)\n",
        "\n",
        "    for t in (bar := trange(T-1, -1, -1)):\n",
        "\n",
        "        bar.set_description(f'[Denoising] step: {t}')\n",
        "        # Sampling\n",
        "        eps = torch.randn(gen_N, chan, *resolu, device=device)\n",
        "        # Denoising\n",
        "        X_T = ALPHA[t]*X_T + (1-ALPHA[t])*model(X_T, t+1) + torch.sqrt(1-ALPHA[t]**2)*eps\n",
        "\n",
        "        if t < 1:\n",
        "            L.append(X_T)\n",
        "            break\n",
        "    \n",
        "    save_image(torch.cat(L)/2+0.5, 'L.jpg')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PPBzb-GFyeK4"
      },
      "source": [
        "### 1.2.a \n",
        "#### **Draw** some generated samples based on diffusion steps $T = 500$ and $T = 1000$. We provide the **pre-trained weights** which are trained with 500 and 1000 steps. Hint: In the paper, the steps start at 1.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGTyZPkMfCcO",
        "outputId": "03a6c0a7-ba48-481e-8f6b-dfed31a5bd04"
      },
      "outputs": [],
      "source": [
        "# gen_N is the setting of output images number\n",
        "# resolu is the setting of output images resolution, you should not change this.\n",
        "# This function will automatically save the sample images, what you need to do is to show in here.\n",
        "generate_and_save(model1, gen_N=64, resolu=(64, 64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_and_save(model2, gen_N=64, resolu=(64, 64))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XXPkkcliyq_1"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S_E8w2GlyrWO"
      },
      "source": [
        "### 1.2.b\n",
        "#### **Discuss** the result based on different diffusion steps."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "imJ8XghXyyJ4"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jzRUZ8KOlktc"
      },
      "source": [
        "## 1.3 Comparison between GAN and DDPM (10%)\n",
        "#### (a) Both GAN and DDPM are generative models. The following figures are randomly generated results by using GAN (left) and DDPM (right). Please describe the pros and cons of the two models. (10%)\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://i.imgur.com/pU77cfa.jpg\" width=\"600px\"/>\n",
        "</center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MHZqdj48yJ6Q"
      },
      "source": [
        "### 1.3.a\n",
        "### Both GAN and DDPM are generative models. The figures are randomly generated results by using GAN (left) and DDPM (right). Please describe the pros and cons of the two models based on your observation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "68qSH0keyO60"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
