{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data:\n",
      "(4500, 784)\n",
      "(4500,)\n",
      "(500, 784)\n",
      "(500,)\n",
      "shape of weights:\n",
      "(784, 2048)\n",
      "(2048, 512)\n",
      "(512, 5)\n",
      "\n",
      "shape of biases:\n",
      "(2048,)\n",
      "(512,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "# Example code for reading the data and the initial weights and biases.\n",
    "# Note: This is just an example of how to read these files, you can modify the code in your own implementation.\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "train_x, train_y = np.load('train_x.npy'), np.load('train_y.npy')\n",
    "test_x, test_y = np.load('test_x.npy'), np.load('test_y.npy')\n",
    "\n",
    "print('shape of data:')\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "\n",
    "checkpoint = np.load('weights.npy', allow_pickle=True).item()\n",
    "init_weights = checkpoint['w']\n",
    "init_biases = checkpoint['b']\n",
    "\n",
    "print('shape of weights:')\n",
    "for w in init_weights:\n",
    "    print(w.shape)\n",
    "    \n",
    "\n",
    "print()\n",
    "\n",
    "print('shape of biases:')\n",
    "for b in init_biases:\n",
    "    print(b.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1\n",
    "<style>\n",
    "    .red {\n",
    "        color: red;\n",
    "    }\n",
    "    .blue {\n",
    "        color: skyblue;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "Design a FNN model architecture and use the file of the initial weights and biases “<span class=\"blue\">weights.npy</span>”. \n",
    "\n",
    "Run the <span class=\"red\">backpropagation</span> algorithm and use the <span class=\"red\">mini-batch SGD</span> (stochastic gradient descent) \n",
    "$$\n",
    "    \\mathbf{w}^{(\\tau+1)}=\\mathbf{w}^{(\\tau)}-\\eta \\nabla J\\left(\\mathbf{w}^{(\\tau)}\\right)\n",
    "$$\n",
    "to optimize the parameters (<span class=\"blue\">the weights and biases)</span>,\n",
    "where $\\nabla$ is the learning rate. \n",
    "\n",
    "<span class=\"red\">You should implement the FNN training under the following settings:</span>\n",
    "\n",
    "- number of layers: 3\n",
    "- number of neurons in each layer (in order): 2048, 512, 5\n",
    "- activation function for each layer (in order): relu, relu, softmax\n",
    "- number of training epochs: 30\n",
    "- learning rate: 0.01\n",
    "- batch size: 200\n",
    "- **important note**: For 1(a), <span class=\"red\">DO NOT RESHUFFLE THE DATA.</span> We had already shuffled the data for you.\n",
    "\n",
    "Reshuffling will make <span class=\"blue\">your result differ from our ground-truth result</span>, and <span class=\"red\">any difference will result in reduction of your points.</span>\n",
    "\n",
    "On the same note, when splitting the samples into batches, split them in the given sample order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .red {\n",
    "        color: red;\n",
    "    }\n",
    "    .blue {\n",
    "        color: skyblue;\n",
    "    }\n",
    "</style>\n",
    "(a) **Plot** the <span class=\"blue\">learning curves</span> of $J(\\mathbf{w})$ and the <span class=\"blue\">accuracy</span> of classification <span class=\"blue\">for every 25 iterations</span>, with training data as well as test data, also, **show** the final loss and accuracy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for feedfarward neural network\n",
    "class FeedforwardNeuralNetwork:\n",
    "    def __init__(self, init_weights, init_biases, lr = 0.01, epoch = 30, batch_size = 200):\n",
    "        self.weights = init_weights\n",
    "        self.biases = init_biases\n",
    "        self.num_layers = len(init_weights)\n",
    "        self.num_neurons = [b.shape for b in init_biases]\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = np.dot(x, self.weights[i]) + self.biases[i]\n",
    "            # relu as the activation function\n",
    "            if i != self.num_layers - 1:\n",
    "                x = np.maximum(x, 0)\n",
    "        return x\n",
    "        \n",
    "    def backpropagation(self, x, y):\n",
    "        # calculate the gradient\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        # forward\n",
    "        output_y = self.forward_propagation(x)\n",
    "        pred_y = np.argmax(output_y, axis = 1)\n",
    "        # backward\n",
    "        for i in range(self.num_layers - 1, -1, -1):\n",
    "            if i == self.num_layers - 1:\n",
    "                grad_w[i] = np.dot(x.T, pred_y - y)\n",
    "                grad_b[i] = np.sum(pred_y - y, axis = 0, keepdims = True)\n",
    "            else:\n",
    "                grad_w[i] = np.dot\n",
    "                grad_b[i] = np.sum(np.dot(pred_y - y, self.weights[i + 1].T) * (x > 0), axis = 0, keepdims = True)\n",
    "        return grad_w, grad_b\n",
    "        \n",
    "    # train the train data but without shuffle\n",
    "    def train_model(self, train_x, train_y):\n",
    "        for i in range(self.epoch):\n",
    "            for j in range(0, len(train_x), self.batch_size):\n",
    "                batch_x = train_x[j:j+self.batch_size]\n",
    "                batch_y = train_y[j:j+self.batch_size]\n",
    "                # backward\n",
    "                grad_w, grad_b = self.backpropagation(batch_x, batch_y)\n",
    "                # update the weights and biases\n",
    "                for k in range(self.num_layers):\n",
    "                    self.weights[k] -= self.lr * grad_w[k]\n",
    "                    self.biases[k] -= self.lr * grad_b[k]\n",
    "                # calculate the loss\n",
    "                loss = np.sum(np.square(np.dot(train_x, self.weights[0]) + self.biases[0]) - train_y)\n",
    "                # print the loss\n",
    "                print(\"Epoch: %d, Batch: %d, Loss: %f\" % (i, j, loss))\n",
    "                self.loss.append(loss)\n",
    "                # predict the train data based on the weights and biases\n",
    "                output_y = np.dot(train_x, self.weights[0]) + self.biases[0]\n",
    "                # output the class with the highest probability\n",
    "                pred_y = np.argmax(output_y, axis = 1)\n",
    "                # accuracy\n",
    "                acc = np.sum(np.argmax(pred_y, axis = 1) == np.argmax(train_y, axis = 1) / len(train_y))\n",
    "                self.accuracy.append(acc)\n",
    "                print(\"Accuracy: %f\" % acc)\n",
    "        return self.weights, self.biases\n",
    "    \n",
    "    # test the test data\n",
    "    def test_model(self, test_x, test_y):\n",
    "        # use the trained weights and biases to predict the test data\n",
    "        for i in range(self.num_layers):\n",
    "            test_x = np.dot(test_x, self.weights[i]) + self.biases[i]\n",
    "            # use relu as the activation function\n",
    "            if i != self.num_layers - 1:\n",
    "                test_x = np.maximum(test_x, 0)\n",
    "            # softmax with 5 classes\n",
    "            test_x = np.exp(text_x)\n",
    "            test_x = test_x / np.sum(text_x, axis = 1, keepdims = True)\n",
    "            acc = np.sum(np.argmax(test_x, axis = 1) == np.argmax(test_y, axis = 1) / len(test_x))\n",
    "            print(\"Accuracy: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[138], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m FNN \u001b[39m=\u001b[39m FeedforwardNeuralNetwork(init_weights, init_biases, lr \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m, epoch \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m, batch_size \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m FNN\u001b[39m.\u001b[39;49mtrain_model(train_x, train_y)\n\u001b[0;32m      4\u001b[0m \u001b[39m# test the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m FNN\u001b[39m.\u001b[39mtest_model(test_x, test_y)\n",
      "Cell \u001b[1;32mIn[137], line 46\u001b[0m, in \u001b[0;36mFeedforwardNeuralNetwork.train_model\u001b[1;34m(self, train_x, train_y)\u001b[0m\n\u001b[0;32m     44\u001b[0m batch_y \u001b[39m=\u001b[39m train_y[j:j\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size]\n\u001b[0;32m     45\u001b[0m \u001b[39m# backward\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m grad_w, grad_b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackpropagation(batch_x, batch_y)\n\u001b[0;32m     47\u001b[0m \u001b[39m# update the weights and biases\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n",
      "Cell \u001b[1;32mIn[137], line 35\u001b[0m, in \u001b[0;36mFeedforwardNeuralNetwork.backpropagation\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     33\u001b[0m         grad_b[i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(pred_y \u001b[39m-\u001b[39m y, axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, keepdims \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m         grad_w[i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(x, np\u001b[39m.\u001b[39;49mdot(pred_y \u001b[39m-\u001b[39;49m y), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mT) \u001b[39m*\u001b[39m (x \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m         grad_b[i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mdot(pred_y \u001b[39m-\u001b[39m y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mT) \u001b[39m*\u001b[39m (x \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m), axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, keepdims \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m grad_w, grad_b\n",
      "File \u001b[1;32m<__array_function__ internals>:4\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dot() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "FNN = FeedforwardNeuralNetwork(init_weights, init_biases, lr = 0.01, epoch = 30, batch_size = 200)\n",
    "# train the model\n",
    "FNN.train_model(train_x, train_y)\n",
    "# test the model\n",
    "FNN.test_model(test_x, test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .red {\n",
    "        color: red;\n",
    "    }\n",
    "    .blue {\n",
    "        color: skyblue;\n",
    "    }\n",
    "</style>\n",
    "(b) **Repeat 1(a)** by considering <span class=\"red\">zero initialization</span> for the model weights. And **make some discussion.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2\n",
    "<style>\n",
    "    .red {\n",
    "        color: red;\n",
    "    }\n",
    "    .blue {\n",
    "        color: skyblue;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "Based on the model in 1, please <span class=\"blue\">implement the dropout layers</span> and apply them <span class=\"blue\">after the first two hidden layers</span>, i.e. the layers with 2048 and 512 neurons. \n",
    "\n",
    "The <span class=\"blue\">dropout rate should be set as 0.2</span> for both layers. \n",
    "\n",
    "Note that the dropout operation <span class=\"blue\">should only be applied in the training phase</span> and should be disabled in the test phase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) **Train** the model by using the same settings in 1 and **repeat 1(a).**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Based on the experimental results, how the dropout layers affect the model performance and why? Please **make some discussion.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3\n",
    "\n",
    "Based on the model in 1, please implement mini-batch SGD (stochastic gradient descent).\n",
    "\n",
    "In this problem, we need to reshuffle the data in every batch. Note that the other settings remain the same. \n",
    "\n",
    "Please set the random seed as **42**, and please use **random** library that we have imported."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .red {\n",
    "        color: red;\n",
    "    }\n",
    "    .blue {\n",
    "        color: skyblue;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "(a) **Plot** the <span class=\"blue\">learning curves</span> of $J(\\mathbf{w})$ and the classification <span class=\"blue\">accuracy for every 25 iterations.</span> Please **show** the final values of loss and accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .red {\n",
    "        color: red;\n",
    "    }\n",
    "    .blue {\n",
    "        color: skyblue;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "(b) Based on the experimental results, how the <span class=\"blue\">process of reshuffling images</span> affects the model performance and why? Please **make some discussion.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77efda19133223beae4d8202f64c0ed7ef77e02bd33099328534a5c00204ca3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
